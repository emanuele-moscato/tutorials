{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras layers with TensorFlow\n",
    "\n",
    "The Keras library provides abstractions for building neural networks in Python. It can leverage different deep learning libraries as the backend, effectively providing a standardized interface in top of them in the form of an API.\n",
    "\n",
    "In this tutorial, the use of Keras layers within a TensorFlow workflow is demonstrated. In order to be able to use Keras you have to install it first (`pip install keras` or applying the `keras` environment to the SherlockML server that runs this notebook).\n",
    "\n",
    "Tutorial taken from: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the structure of the graph (neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `placeholder` objects from TensorFlow are meant to be the prototypes of (input) layers of neural network. In this case we are specifying that each datapoint is an array of `float32` numbers with shape (,784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=(None, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the structure of the neural network using `Dense` layers from Keras. Despite these not being TensorFlow objects, they can be made to interact with them, and TensorFlow syntax can be used. In this case the structure is:\n",
    "$$\n",
    "(\\text{img}) \\longrightarrow \\text{Dense} \\longrightarrow \\text{Dense} \\longrightarrow \\text{Dense},\n",
    "$$\n",
    "with `img` representing the input datapoint. Disregarding the nonlinear activation functions (`relu` and `softmax`), the matrix structure of the neural network is:\n",
    "$$\n",
    "\\biggl[10\\times128\\biggr]\\biggl[128\\times128\\biggr]\\biggl[128\\times784\\biggr]\\biggl(784\\biggr) = \\biggl(10\\biggr),\n",
    "$$\n",
    "where we have denoted $n$-component (column) vectors with $\\biggl(n\\biggr)$ and $m\\times n$ matrices as $\\biggl[n\\times m\\biggr]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another `placeholder` object corresponds to the output (a one-hot encoding of the ten digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has specific objects corresponding to different possible loss functions to use. In this case we use the categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset (available within TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides `operation` objects corresponding to abstractions of operations. In graph language operations (or \"ops\") correspond to the nodes of the graph. In this case we define a training step, corresponding to optimizing the weights of the neural network using gradient descent to minimize the previously defined loss function.\n",
    "\n",
    "The general structure is:\n",
    "- `tensor`s correspond to edges in the graph. They have no values by themselves, but can be evaluated.\n",
    "- `operation`s correspond to nodes in the graphs and describe calculations that consume and produce tensors.\n",
    "- `session`s run TensorFlow operations and encapsulate the state of the TensorFlow runtime.\n",
    "\n",
    "For more information on operations, tensors and sessions see: https://www.tensorflow.org/programmers_guide/low_level_intro\n",
    "\n",
    "There is no need to explicitly define __stochastic__ gradient descent, as this is achieved just by passing only a subset (*batch*) of the whole dataset to a regular gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow variables need to be initialized if we are using the low-level API (on the other hand high level abstractions like Keras automatically initialize the variables).\n",
    "\n",
    "The `global_variables_initializer` function returns an operation (to be run by a `session`) that initializes all the global variables in the `tf.GraphKeys.GLOBAL_VARIABLES` variables collection in one go. Operations are objects that can be passed to the `run()` method of a `session` to be executed.\n",
    "\n",
    "For more information see: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already conveniently devided into batches of which we can specify the size. We loop over all the batches, performing a training step (gradient descent) at each iteration. The syntax below uses the `session` as a context variable: this is just a convenient way to use an operation's `run()` method. In fact, all operations are executed by the session.\n",
    "\n",
    "Although we used Keras layers, all the operations are performed by TensorFlow objects, so the training phase doesn't give the nice visual feedback we get when using Keras...\n",
    "\n",
    "From a TensorFlow perspective, what we are running the `train_step` operation and for each batch of data we pass the features to the `img` placeholder (tat goes into the input layer of the graph) and the labels to the `labels` placeholder. This is required by the definition of `train_step`, because the `loss` cost function has `preds` (output layer of the graph) and `labels` (placeholder for the true labels). Indeed, the output layer of the graph, once evaluated passing a datapoint as the input, produces the predicted labels: as usual the const function depends on the true labels and the predicted ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TensorFlow operations\n",
    "\n",
    "TensorFlow `operation`s (the nodes of the graph) are always run by a TensorFlow `session`. This can be done in either of three ways:\n",
    "- Calling the session's `run()` method, passing it the operation. If the operation requires arguments to be specified, they are passed as a dictionary called `feed_dict`.\n",
    "- Calling the operation's `run()` method with the session open as a context variable (obtained by calling the `session.as_default()` method).\n",
    "- Calling the operation's `run()` method, passing it the session as the `session` keyword argument, plus the `feed_dict` dictionary if needed.\n",
    "\n",
    "The cells below demonstrate these three ways of doing the same thing. Notice that executing more than one cell correspond to more training (more epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the session's run() method.\n",
    "for i in range(100):\n",
    "    batch = mnist_data.train.next_batch(50)\n",
    "    sess.run(\n",
    "        train_step,\n",
    "        feed_dict={\n",
    "            img: batch[0],\n",
    "            labels: batch[1]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the session as a context variable.\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(\n",
    "            feed_dict={\n",
    "                img: batch[0],\n",
    "                labels: batch[1]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the operation's run() method.\n",
    "for i in range(100):\n",
    "    batch = mnist_data.train.next_batch(50)\n",
    "    train_step.run(\n",
    "        feed_dict={\n",
    "            img: batch[0],\n",
    "            labels: batch[1]\n",
    "        },\n",
    "        session=sess\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides categorical accuracy (a generalization of the accuracy metric to the multi-class case) as a possible metric for testing. \n",
    "\n",
    "When passed TensorFlow tensors corresponding to the labels and the predictions (`labels` is the placeholder for the one-hot encoded digits, `pred` is the output layer of the graph), the categorical accuracy becomes a tensor itself, which can be evaluated with using its `eval()` method.\n",
    "\n",
    "### Evaluating tensors\n",
    "\n",
    "Tensors are evaluated in pretty much the same way operations are run: if they depend on other parts of the graph whose values are stored in a session, they need a session to be evaluated. This can happen in three possible ways:\n",
    "- Pass the tensor to the session's `run()` method, along with any needed argument (e.g. the features of a datapoint) in the `feed_dict` dictionary.\n",
    "- Call the `eval()` method of the tensor with the session open as a context variable, again passing any needed argument in `feed_dict`.\n",
    "- Call the `eval()` method of the tensor, passing the session to it, along with the arguments.\n",
    "\n",
    "In the test phase, we evaluate the categorical accuracy passing the 10000 test images as datapoints (`preds` will give a prediction for each) and their labels as the true labels w.r.t. which to compute the accuracy metric.\n",
    "\n",
    "The output of the evaluation is a `NumPy` with a number of components equal to the number of test datapoints. Each component contains either 1 (correct prediction) or 0 (wrong prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_value = accuracy(labels, preds)\n",
    "\n",
    "with sess.as_default():\n",
    "    acc = acc_value.eval(\n",
    "        feed_dict={\n",
    "            img: mnist_data.test.images,\n",
    "            labels: mnist_data.test.labels\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(acc, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Categorical accuracy: {}%\".format(100*(acc.sum()/len(acc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the prediction for a single datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the prediction for a single datapoint from the neural network by evaluating the `preds` tensor and passing one image from the dataset as the input.\n",
    "\n",
    "A tensor is a somewhoat immaterial construct: tensors always need a `session` to be evaluated. When using the `eval()` method of a tensor, a session must always be specified: this is just a shortcut to avoid typing `session.run()`. A session can be passed in the keyword argument to `eval()` or can be specified as a Python context variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the session as context variable.\n",
    "with sess.as_default():\n",
    "    one_pred = preds.eval(\n",
    "        feed_dict={\n",
    "            img: mnist_data.train.images[0].reshape((1,784))\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are equivalent to the above one and demonstrates how a session is needed to evaluate a tensor, if the tensor depends on other parts of the graph, and the possible syntax to make that happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the session to the eval() method of the tensor.\n",
    "one_pred = preds.eval(\n",
    "    feed_dict={\n",
    "        img: mnist_data.train.images[0].reshape((1,784))\n",
    "    },\n",
    "    session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the tensor to the run() method of the session.\n",
    "sess.run(\n",
    "    preds,\n",
    "    feed_dict={\n",
    "        img: mnist_data.train.images[0].reshape((1,784))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted digit: \"+str(np.argmax(one_pred[0])))\n",
    "print(\"Probability: \"+str(one_pred[0][np.argmax(one_pred[0])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
