{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the dataset. We work with one-dimensional feature vectors, with feature values generated uniformly in $[-5,5]$. The target values will be normally distributed (mean $\\mu = 0$, standard deviation $\\sigma=50$) around a cubic function of the feature values, with the law\n",
    "\n",
    "$$\n",
    "y = F(x) = x^3 - \\frac{1}{2}\\,x^2 + 20\\,x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-5.0, 5.0, size=(1000,1))\n",
    "Y = np.random.normal(X**3 - 5.0*X**2 + 20.0*X, 50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = X[:,0],\n",
    "    y = Y[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = dict(\n",
    "        title = 'x'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'y'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offers an high-level interface to the underlying backend (TensorFlow in this case). A neural network is Keras is defined as a `model` object to which we can add layers to build the graph. Here we work with a feed-forward neural network built with fully connected `Dense` layers.\n",
    "\n",
    "For general advice on nonlinear regression using Keras, see:\n",
    "- https://github.com/keras-team/keras/issues/1874\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first layer of the graph the `input_shape` keyword argument must be specified. This is a tuple of integers and does __not__ include the sample axis. Keras expects the input to have the form (n_samples, \\[features\\]), and `input_shape` must correspond to the shape of the features tensor.\n",
    "\n",
    "__Example:__ if we have 50 samples, each of which is an array (tensor of rank 1 in TensorFlow terms) with 10 components, we must specify `input_shape=(10,)`.\n",
    "\n",
    "The dimension of the last layer decides the shape of the output. Here we start with 1-dimensional inputs and we end up with 1-dimensional outputs, as we have a function $F: \\mathbb{R}\\to\\mathbb{R}$. The hidden (intermediate) layers, on the other hand, are wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common issue when doing regression with neural network is the __exploding gradients problem__, where the loss function diverges during the training phase. In this case, it was sufficient to switch from stochastic gradient descent to an Adam optimizer to solve that. The Adam optimizer is a generalization of stochastic gradient descent that exploits an adaptive learning rate.\n",
    "\n",
    "For information about the exploding gradients problem, see:\n",
    "- https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network\n",
    "- http://neuralnetworksanddeeplearning.com/chap5.html\n",
    "\n",
    "For information about the Adam optimizer, see: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "\n",
    "To initialize the neural network we just call a model's `compile()` method. This include the initialization of all the weights of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model's `summary()` method gives an overview of the structure of the graph. In particular, from this we can see the number of parameters the neural network possesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the neural network to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides an interface for training and predicting completely analogous to that of a SKLearn model. Among the options, we can specify the `epochs` parameters.\n",
    "\n",
    "__Epochs:__ number of passages over the whole dataset the neural network will do in the training phase.\n",
    "\n",
    "Since the dataset is small we are not dividing it in batches, meaning that each gradient descent (or Adam) step uses the exact gradient descent. This also implies that there are no stochastic features in the path followed in parameters space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a new dataset and get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the neural network did a good job, we can make prediction over a (1-dimensional) grid of values and compare it with the true value of the function to fit on the same grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.linspace(-10.0, 10.0, 1000).reshape((1000,1))\n",
    "Y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_to_fit(x):\n",
    "    return x**3 - 5.0*x**2 + 20.0*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_data = go.Scatter(\n",
    "    x = X[:,0],\n",
    "    y = Y[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'data'\n",
    ")\n",
    "\n",
    "trace_pred = go.Scatter(\n",
    "    x = X_pred[:,0],\n",
    "    y = Y_pred[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'predictions'\n",
    ")\n",
    "\n",
    "trace_true = go.Scatter(\n",
    "    x = X_pred[:,0],\n",
    "    y = f_to_fit(X_pred)[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'true values'\n",
    ")\n",
    "\n",
    "data = [trace_data, trace_pred, trace_true]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = dict(\n",
    "        title = 'x'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'y'\n",
    "    ),\n",
    "    hovermode = 'closest'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments: overfitting?\n",
    "\n",
    "A question is in order on whether by looking at the plot above we should think we are overfitting the data. Notice that no cross validation (train-test split) has been done: we are just comparing the predictions from the neural network with the true, deterministic form of the function we wanted to fit. Ultimately, the question we would like to answer is: __given the data, can the neural network learn the true form of the function?__\n",
    "\n",
    "In the region where datapoints are present, the fit to the data is good and there's good correspondence with the deterministic function as well: to see if there's overfitting here we should do cross validation on the dataset.\n",
    "\n",
    "In the region where no datapoint is present, the predictions do not match the deterministic function. In my view, this is somewhat expected: approximating functions with functions of a different form usually requires series (power series, trigonometric series etc.). The prediction of the neural networks, as a function of the input, is just not a cubic, therefore shouldn't be expected to match it in an arbitrary large domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tweak the topology of the graph and the number of epochs to see how the results vary. The best generalization seems to be the one using wider layers with `tanh` activation function followed by narrower linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(1, input_shape=(1,)))\n",
    "model_2.add(Dense(10, activation='tanh'))\n",
    "model_2.add(Dense(5))\n",
    "model_2.add(Dense(10, activation='tanh'))\n",
    "model_2.add(Dense(5))\n",
    "model_2.add(Dense(10, activation='tanh'))\n",
    "model_2.add(Dense(5))\n",
    "model_2.add(Dense(1))\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "model_2.fit(X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_2.predict(X_pred)\n",
    "\n",
    "trace_data = go.Scatter(\n",
    "    x = X[:,0],\n",
    "    y = Y[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'data'\n",
    ")\n",
    "\n",
    "trace_pred = go.Scatter(\n",
    "    x = X_pred[:,0],\n",
    "    y = Y_pred[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'predictions'\n",
    ")\n",
    "\n",
    "trace_true = go.Scatter(\n",
    "    x = X_pred[:,0],\n",
    "    y = f_to_fit(X_pred)[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'true values'\n",
    ")\n",
    "\n",
    "data = [trace_data, trace_pred, trace_true]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = dict(\n",
    "        title = 'x'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'y'\n",
    "    ),\n",
    "    hovermode = 'closest'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on no-noise data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if, instead of generating data with target values distributed (normally) around a deterministic function of the feature values, we train the neural network with data exactly following that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_det = np.linspace(-5.0, 5.0, 1000).reshape((1000,1))\n",
    "Y_det = f_to_fit(X_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = X_det[:,0],\n",
    "    y = Y_det[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = dict(\n",
    "        title = 'x'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'y'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_det = Sequential()\n",
    "model_det.add(Dense(1, input_shape=(1,)))\n",
    "model_det.add(Dense(10, activation='tanh'))\n",
    "model_det.add(Dense(5))\n",
    "model_det.add(Dense(10, activation='tanh'))\n",
    "model_det.add(Dense(5))\n",
    "model_det.add(Dense(10, activation='tanh'))\n",
    "model_det.add(Dense(5))\n",
    "model_det.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_det.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "model_det.fit(X_det, Y_det, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pred = np.linspace(-10.0, 10.0, 1000).reshape((1000,1))\n",
    "Y_pred = model_det.predict(X_pred)\n",
    "\n",
    "trace_data = go.Scatter(\n",
    "    x = X_det[:,0],\n",
    "    y = Y_det[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'data'\n",
    ")\n",
    "\n",
    "trace_pred = go.Scatter(\n",
    "    x = X_pred[:,0],\n",
    "    y = Y_pred[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'predictions'\n",
    ")\n",
    "\n",
    "trace_true = go.Scatter(\n",
    "    x = X_pred[:,0],\n",
    "    y = f_to_fit(X_pred)[:,0],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 3\n",
    "    ),\n",
    "    name = 'true values'\n",
    ")\n",
    "\n",
    "data = [trace_data, trace_pred, trace_true]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = dict(\n",
    "        title = 'x'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'y'\n",
    "    ),\n",
    "    hovermode = 'closest'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Even with data completely devoid of noise, the neural network provides predictions that do not match the true function outside of the region where the datapoint are present. This again signifies that the model does a good job at fitting what it sees but hasn't learned the true underlying rule: you need a cubic to fully understand a cubic!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
