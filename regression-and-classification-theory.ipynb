{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces basic regression and classification techniques (linear regression and logistic regression respectively) using Python. Rather than focusing on how to analyse real-world data, we present a discussion of the theoretical aspects the techniques are based on.\n",
    "\n",
    "Some details are included about syntax and implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Jupyter notebooks (.ipynb)\n",
    "Python is an interpreted programming language: there is no source code to pass to a compiler, but rather instructions that are executed runtime by an interpreter. Jupyer notebooks are a good way to write Python code in an interactive way very similar to what is used in Wolfram's Mathematica notebooks.\n",
    "\n",
    "A Jupyter notebook is structured in cells, either with text (\"markdown cells\") or with code. Once you select a cell, you can edit it by pressing `enter`. If you are editing a cell and you want to go back to scrolling among cells, press `esc`. In order to execute a cell, you just have to press `shift+enter`. Executing markdown cells just formats the text in the right way ($\\LaTeX$ expressions can be included), while executing code cells executes the code.\n",
    "\n",
    "The code is run by a `kernel` that remembers the code in the chronological order in which the cells are executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "Python is good because it has many open source libraries that help us do many of the most common tasks without having to implement algorithms and data structures ourselves. The following are the main packages we usually use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Let's assume we have M datapoints $(x_1, y_1), \\ldots, (x_M, y_M)$, where each $x_i$ is a continuous input variable (also called \"feature\") and $y_i$ is a corresponding output variable (also called \"target variable\"). Linear regression consists in fitting the best straight line \n",
    "\n",
    "$$\n",
    "y = m\\,x + q\n",
    "$$\n",
    "\n",
    "to the datapoints. It would be quite unusual to see data perfectly distributed on a straight line: indeed we assume that the datapoints are distributed around the above law with a Gaussian (normal) probability distribution with mean $\\mu$ and standard deviation $\\sigma$. Under this assumption, the probability of observing a datapoint $(x_i, y_i)$ given certain values of the parameters $(\\mu, \\sigma)$ is\n",
    "\n",
    "$$\n",
    "p\\left( (x_i, y_i) | (\\mu, \\sigma) \\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\,\\exp\\left( - \\frac{\\left(y_i - (m\\,x_i+q)\\right)^2}{2\\sigma^2} \\right).\n",
    "$$\n",
    "\n",
    "This is called the *likelihood* of the datapoint $(x_i, y_i)$. The expression that relates $x_i$ and $y_i$ contains the parameters $m$ and $q$, and obtaining the best straight line means finding the optimal values for the parameters given the data. To do so, we maximize the likelihood of the whole dataset within respect to the values of the parameters themselves. Assuming uniform probability distribution of the parameters and that the samples are indipendent and identically distributed, the optimal values for the parameters are found minimizing the Mean Squared Error,\n",
    "\n",
    "$$\n",
    "MSE(m, q) \\equiv \\frac{1}{2 M} \\sum_{i=1}^M \\left(y_i - (m\\,x_i + q) \\right)^2.\n",
    "$$\n",
    "\n",
    "Notice that assuming that the likelihood of a sample has the form $p\\left( (x_i, y_i) | (\\mu, \\sigma) \\right)$ is equivalent to stating that $y_i$ is related to $x_i$ thorugh a linear function plus some normally distributed noise $\\epsilon(\\mu, \\sigma)$,\n",
    "\n",
    "$$\n",
    "y_i = m\\,x_i + q + \\epsilon(\\mu, \\sigma).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 50.0*np.random.random(size=100) - 10.0\n",
    "\n",
    "Y = np.random.normal(loc=1.25*X + 3.3, scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, color='r')\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.title('Data', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function returning the MSE given the data. Remember that the MSE is a function of the parameters $m$ and $q$: here the data is thought of as given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(params):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    List or 1D array of parameters. First entry is m,\n",
    "    second entry is q.\n",
    "           \n",
    "    RETURNS\n",
    "    -------\n",
    "    Mean square error computed on the dataset, given\n",
    "    the cvalues for the parameters from the input.\n",
    "    \"\"\"\n",
    "    v = (Y - (params[0]*X + params[1]))**2\n",
    "    \n",
    "    v = v.sum()/(2.0*len(X))\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 3D surface given by\n",
    "$$\n",
    "z = MSE(m, q),\n",
    "$$\n",
    "for $m$ and $q$ ranging in some intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms, qs = np.meshgrid(np.linspace(-2., 2., 100), np.linspace(1., 5., 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = np.array([mse([m, q]) for m, q in zip(np.ravel(ms), np.ravel(qs))])\n",
    "\n",
    "mses = mses.reshape(ms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(ms, qs, mses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a minimization algorithm (in the form of a Python class) from the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `minimize` function wants as the input the function it has to minimize and an initial guess for the values of the vaiables on which to minimize it (in our case, the parameters $m$ and $q$). We can also specify a minimization method (the specific algorithm to be used): if we don't say anything the defaulf one is BFGS. Some methods require the gradient of the function to be specified, but since we can't be bothered conputing it we choose the Nelder-Mead method, which does not require anything but the function to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_guess = 0.0\n",
    "q_guess = 0.0\n",
    "\n",
    "guesses = [m_guess, q_guess]\n",
    "\n",
    "res = minimize(mse, guesses, method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minimize` returns a Python class in which the results of the minimization procedure are stored. In particular, the value of the variables that minimize the function are stored in the `x` attribute of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the MSE on the optimized value for $m$ and $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the line we found and the data to see whether there is a qualitative agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = np.arange(-15., 45., 0.2)\n",
    "Y_line = res.x[0]*X_line + res.x[1]\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "plt.plot(X_line, Y_line, color='r', linewidth=3)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using `sklearn`\n",
    "Having to define the MSE and minimizing it with the algorithm from the `scipy` library is not difficult, but if we want to do a linear regression the `sklearn` library offers us a built-in standardized way that's even easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the linear regression model from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce the default syntax for the machine learning models in `sklearn` (notice that linear regression tunes the parameters of a model on some given data, and therefore **IS** a machine learning algorithm!).\n",
    "\n",
    "A machine learning model imported from `sklearn` is a Python class, so the first thing to do is to instantiate an object belonging to that class.\n",
    "\n",
    "Example: if we define the class `Human`, we define the general blueprint for an object, but no human has been created yet. If we then say `ema = Human()`, we are instantiating an object whose class is `Human`, and indeed we have such an object at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work with the attributes (variables) and methods (functions) inside the `LinearRegression` instance `linreg`.\n",
    "\n",
    "There are two main methods inside all of the `sklearn` models: `fit` and `predict`.\n",
    "\n",
    "- `fit` does the learning: it has the training data (the samples for which both the input and the output is known, as in the pairs $(x_i, y_i)$) as the input and it tunes the parameters to best fit it.\n",
    "\n",
    "- `predict` is the one that predicts the output of new data for which only the features are known. We haven't seen this above, but in most practical applications we are not just interested in the parameters of the model that best fit some data: what we want is to set the parameter to the optimized value and then use the model to make predictions on new, previously unseen data. We'll see an example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model to the data, we first have to racast the arrays `X` and `Y` in which we are storing the training data in the right shape. In a nutshell, the latest version of `sklearn` wants the data to be put into arrays with shape (n_samples, n_features), so in our case it would be arrays of shape (100, 1). The problem is that we defined `X` and `Y` as arrays of shape (100) (the number of components is of course the same).\n",
    "\n",
    "To solve the problem we have to reshape the arrays calling the `reshape` method of the arrays themselves (notice that this doesn't permanently change the shape of the original array!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass the reshaped arrays to the `fit` method of `linreg` to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X.reshape(-1,1), Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the parameters are stored in the `coef_` and `intercept_` attributes of `linreg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_, linreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a new dataset for whose samples we only know the vaues of the features (the $x_i$) and let's use our trained model to find the corresponding output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new samples (features only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = (20.0*np.random.random(20) + 60.0).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output values for the new samples using the trained linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = linreg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = np.arange(-10.0, 90.0, 0.2)\n",
    "Y_line = linreg.coef_[0,0]*X_line + linreg.intercept_\n",
    "\n",
    "plt.scatter(X, Y, color='g', label='Training data')\n",
    "plt.scatter(X_new, Y_predicted, color='b', label='New data')\n",
    "plt.plot(X_line, Y_line, color='r', label='Trained model')\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is not that different from regression apart from the fact that the output variables are \"categorical\" (i.e. assume discrete values, called \"class labels\"). In the following we will use a method called __logistic regression__, which we train on some data and use to predict the labels of new data. Although the name says \"regression\", this model is a classifier (the reason for the misnomen is that it is very similar to a regression task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for classification.\n",
    "\n",
    "`sklearn` contains various datasets that can be used to test different methods: here we will use part of the iris dataset, containing 4 features for 150 samples of 3 different categories of iris flowers (50 samples for each category).\n",
    "\n",
    "In this case a datapoint is in the form $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i = (x_i^1, x_i^2, x_i^3, x_i^4)$ is a 4-component vector containing the values of the features of the $i$-th sample, while $y_i$ is a class label.\n",
    "\n",
    "To simplify things further, we only use two classes of flowers, for a total of 100 samples, and only one feature (the one that best distinguishes between the two chosen classes), so we go back to having datapoints with the structure $(x_i, y_i)$, where $x_i$ is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class = load_iris().data[:100,2]\n",
    "Y_class = load_iris().target[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the class label (0 or 1) against the single feature we have for each of the samples.\n",
    "\n",
    "The feature we chose already distinguishes well between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_class, Y_class)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more interesting, we split the samples (both the feature array X and the target array Y) between a training dataset on which we train the model and a test dataset on which we test its accuracy.\n",
    "\n",
    "The procedure of splitting the dataset into a train and a test sub-dataset (plus a validation dataset if needed) is called *cross validation* and is taken care of in a random way by the appropriate functions inside SKlearn (e.g. `train_test_split`). Here we proceed manually for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class_train = np.concatenate((X_class[:40], X_class[50:90]))\n",
    "Y_class_train = np.concatenate((Y_class[:40], Y_class[50:90]))\n",
    "\n",
    "X_class_test = np.concatenate((X_class[40:50], X_class[90:100]))\n",
    "Y_class_test = np.concatenate((Y_class[40:50], Y_class[90:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by doing things by hand: we proceed as in the regression case, but now the function we want to fit to the data is not a straight line, but rather the composition of a straight line\n",
    "\n",
    "$$\n",
    "F(x) = m\\,x + q\n",
    "$$\n",
    "\n",
    "with a sigmoid function\n",
    "\n",
    "$$\n",
    "G(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "In total we fit the function\n",
    "\n",
    "$$\n",
    "y = G(F(x)) = G(m\\,x + q) = \\frac{1}{1+e^{-(m\\,x+q)}}.\n",
    "$$\n",
    "\n",
    "The parameters we want to optimize are again $m$ and $q$: logistic regression is very similar to linear regression after all.\n",
    "\n",
    "We may ask why such a function. One reason is that if we plot the function above we see that it (potentially) interpolates well the plot of the data we had before. The other is more subtle: given a value $x_i$ we interpret $G(F(x_i))$ as the probability of the $i$-th sample to belong to class $y_i=1$,\n",
    "\n",
    "$$\n",
    "P(y_i = 1) = \\frac{1}{1+e^{-(m\\,x_i+q)}}.\n",
    "$$\n",
    "\n",
    "$G$ is just a function that takes a straight line ($F$) as the input and \"squashes\" it between 0 and 1, which is right what we need if we want to interpret the output as a probability.\n",
    "\n",
    "Notice that this only gives the reason $G$ why must be a sigmoid function, but it poses no constraints on $F$: indeed we take $F$ to be linear just because it's the simplest thing we can do, but a priori we might use any form for $F$.\n",
    "\n",
    "In the case of classification tasks, we do not assume that the likelihood of one datapoint is Gaussian, so maximizing the likelihood doesn't lead to the minimization of the MSE. Instead, the probability of a class is assumed to have a __categorical distribution__, which in the case of two classes (binary classification) corresponds to a Bernoulli distribution,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p\\left(y_i|x_i,\\theta\\right) &= \\theta^{\\mathbb{I}(y_i=1)} (1-\\theta)^{\\mathbb{I}(y_i=0)} \\\\\n",
    "&= y_i\\,\\theta + (1-y_i) (1-\\theta),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}(y_i=y) \\equiv \\delta_{y_i,y}$ is the indicator function. In the case of logistic regression, we put $\\theta = G(F(x))$ as defined above.\n",
    "\n",
    "Under the hypothesis that the samples are independent and identically distributed, maximizing the likelihood of observing the data is equivalent to minimizing the total __cross entropy__ (between the class labels and the model's prediction of their probabilities),\n",
    "\n",
    "$$\n",
    "H = - \\sum_i \\left[ y_i\\,\\log\\left( \\frac{1}{1+e^{-(mx_i+q)}}\\right) + (1-y_i)\\,\\left( 1 - \\log\\left( \\frac{1}{1+e^{-(mx_i+q)}}\\right)\\right) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the linear function, the sigmoid function and the appropriate mean square error computed on the training data using the composition of the two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import exp\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1.0 + exp(-x))\n",
    "\n",
    "def linear_function(x, m, q):\n",
    "    return m*x + q\n",
    "\n",
    "def categorical_cross_entropy(params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    -----\n",
    "    X: samples' features.\n",
    "    Y: samples' classes.\n",
    "    params: array of parameters for the logit function (m and q).\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    Categorical cross entropy computed assuming a logit distribution.\n",
    "    \"\"\"\n",
    "    prob_1 = sigmoid_function(linear_function(X_class_train, params[0], params[1]))\n",
    "    \n",
    "    return -(Y_class_train*prob_1 + (1.0 - Y_class_train)*(1.0 - prob_1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize the categorical cross entropy using the `minimize` function in `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_class = minimize(categorical_cross_entropy, [1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_class.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training data and the trained model, which in this case is just the logistic function with the parameters for the straight line set to the optimal values given by `minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sigmoid = np.arange(0.0, 6.0, 0.1)\n",
    "\n",
    "plt.scatter(X_class_train, Y_class_train, label='Training data')\n",
    "plt.plot(X_sigmoid, sigmoid_function(linear_function(X_sigmoid, res_class.x[0], res_class.x[1])), color='r', label='Trained model')\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clearer is we define the trained model as a function of its own. The trained model is nothing but a the composition of a straight line and a sigmoid function that takes the value of the feature of a sample as the input and computes the probability for the sample to belong to class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_sigmoid(x):\n",
    "    return sigmoid_function(linear_function(x, res_class.x[0], res_class.x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to go from computing class probabilities to predicting class labels (0 or 1), we just follow the simple rule\n",
    "\n",
    "$$\n",
    "\\left\\lbrace\\begin{array}{l}\n",
    "\\text{If}\\,\\,P(x) < 0.5\\,\\,\\text{then}\\,\\,y = 0\\\\\n",
    "\\text{If}\\,\\,P(x) \\geq 0.5\\,\\,\\text{then}\\,\\,y = 1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This can be done by passing the output of the trained model through a quantizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizer(p):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    -----\n",
    "    Array of the samples' probabilities, with len(p)=n_samples.\n",
    "    \n",
    "    RETURNS\n",
    "    -------\n",
    "    Array of labels (0 or 1)\n",
    "    \"\"\"\n",
    "    return (p>=0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we perform on the training data.\n",
    "\n",
    "Compute the label of the training samples. We know these labels already and we used them to train the model. If the model is not too rigid (simple) we expect it to perform well on the training data (to the point of risking overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer(trained_sigmoid(X_class_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantify the performance of the trained model, we can use accuracy, defined as\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of samples correctly labelled}}{\\text{Total number of samples}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    return (y_true==y_pred).sum()/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_accuracy(Y_class_train, quantizer(trained_sigmoid(X_class_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly labels 100% of the training data. It's too early to be happy, as we may be overfitting the training dataset: the real performance of a model is evaluated only on the test dataset.\n",
    "\n",
    "Now we have the trained model, predicting the class labels for the test data is easy: we just compute the probability giving the features of the test samples as the input, and then we quantize them to obtain class labels. We can go further and direclty compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(Y_class_test, quantizer(trained_sigmoid(X_class_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call this a success! 100% of the test datapoints have been labelled correctly by our logistic regression model.\n",
    "\n",
    "Let's plot everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_class_train, Y_class_train, label='Training data (with true class)')\n",
    "plt.scatter(X_class_test, quantizer(trained_sigmoid(X_class_test)), label='Test data (with predicted class)')\n",
    "plt.plot(X_sigmoid, sigmoid_function(linear_function(X_sigmoid, res_class.x[0], res_class.x[1])), color='r', label='Trained model')\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: logistic regression with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, `sklearn` provides a class for logistic regression. The syntax is always the same: the model is trained on the training dataset using the `fit` method and is then used to make prediction on new, unlabelled data using the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiation of the `LogisticRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.fit(X_class_train.reshape(-1,1), Y_class_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model's accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(Y_class_train, logistic_regression.predict(X_class_train.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is labelled with a 100% accuracy.\n",
    "\n",
    "Again, this doesn't say much about the how good is the model, the only exception being if it doesn't perform well on the data it's been trained. If this happens, there's no hope to have good performance on the test data and it's a sign that we are underfitting: the model is too simple (too rigid, too few parameters) to capture the information contained in the data. This happens for example if we try to run a linear regression on nonlinear data.\n",
    "\n",
    "Let's evaluate the accuracy on the the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(Y_class_test, logistic_regression.predict(X_class_test.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot what we obtained from using the `sklearn` library.\n",
    "\n",
    "In order to plot the sigmoid from the `LogisticRegression` model we use its `predict_proba` method. This maps a value of the feature to the probability for the corresponding sample to belong to class 1, which is exactly the interpretation for the sigmoid curve.\n",
    "\n",
    "We see that the sigmoid's shape is different from the one obtained working \"by hand\" with `minimize`, even though this doesn't affect the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_class_train, Y_class_train, label='Training data (with true class)')\n",
    "plt.scatter(X_class_test, logistic_regression.predict(X_class_test.reshape(-1,1)), label='Test data (with predicted class)')\n",
    "plt.plot(X_sigmoid, logistic_regression.predict_proba(X_sigmoid.reshape(-1,1))[:,1], color='r', label='Trained model')\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
